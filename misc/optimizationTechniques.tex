\textbf{Least-squares} minimizes the sum of the squares
of the residuals between a given fitting model and data. Linear least-squares,
where the residuals are linear in the unknowns, has a closed form solution
which can be computed by taking the derivative of the residual with respect to
each unknown and setting it to zero. It is commonly used in the engineering
and applied sciences for fitting polynomial functions. Nonlinear least-squares
typically requires iterative refinement based upon approximating the nonlinear
least-squares with a linear least-squares at each iteration.

\textbf{Gradient descent}  is the industry leading, convex
optimization method for high-dimensional systems. It minimizes residuals
by computing the gradient of a given fitting function. The iterative procedure
updates the solution by moving downhill in the residual space. The Newton-
Raphson method is a one-dimensional version of gradient descent. Since it is
often applied in high-dimensional settings, it is prone to find only local minima.
Critical innovations for big data applications include stochastic gradient
descent and the backpropagation algorithm which makes the optimization
amenable to computing the gradient itself.

\textbf{Alternating descent method (ADM)}  avoids computations
of the gradient by optimizing in one unknown at a time. Thus all unknowns
are held constant while a line search (non-convex optimization) can be
performed in a single variable. This variable is then updated and held constant
while another of the unknowns is updated. The iterative procedure continues
through all unknowns and the iteration procedure is repeated until a desired
level of accuracy is achieved.

\textbf{Augmented Lagrange method (ALM)}  is a class
of algorithms for solving constrained optimization problems. They are similar
to penalty methods in that they replace a constrained optimization problem
by a series of unconstrained problems and add a penalty term to the objective
which helps enforce the desired constraint. ALM adds another term designed
to mimic a Lagrange multiplier. The augmented Lagrangian is not the same as
the method of Lagrange multipliers.

\textbf{Linear program and simplex method} are the workhorse algorithms for convex
optimization. A linear program has an objective function which is linear
in the unknown and the constraints consist of linear inequalities and equalities.
By computing its feasible region, which is a convex polytope, the linear
programming algorithm finds a point in the polyhedron where this function
has the smallest (or largest) value if such a point exists. The simplex method
is a specific iterative technique for linear programs which aims to take a given
basic feasible solution to another basic feasible solution for which the objective
function is smaller, thus producing an iterative procedure for optimizing.
